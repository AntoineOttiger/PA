{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "notebook_path = os.getcwd()\n",
    "parent_path = os.path.abspath(os.path.join(notebook_path, \"..\"))\n",
    "sys.path.append(parent_path)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from modules.dataset_func import get_X_from_sim_grid\n",
    "from modules.dataset_func import get_freqs_from_sim_grid\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antoine\\Documents\\master\\PA\\abacus\\sim_plaque\\ANNs\\tools.py:8: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(csv_path,index_col=False, dtype=str)\n",
      "c:\\Users\\Antoine\\Documents\\master\\PA\\abacus\\sim_plaque\\ANNs\\tools.py:8: ParserWarning: Length of header or names does not match length of data. This leads to a loss of data with index_col=False.\n",
      "  df = pd.read_csv(csv_path,index_col=False, dtype=str)\n"
     ]
    }
   ],
   "source": [
    "#import data\n",
    "\n",
    "sim_grid_path = r\"C:\\Users\\Antoine\\Documents\\master\\PA\\abacus\\sim_plaque\\run_sims\\outputs\\20250326191527\"\n",
    "\n",
    "X = get_X_from_sim_grid(sim_grid_path, [\"distances\", \"rayons\", \"plaque_epaisseurs\"])\n",
    "y = get_freqs_from_sim_grid(sim_grid_path, mode=\"1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25.   4.5  1. ]\n",
      " [25.   3.   1. ]]\n",
      "[[ 0.  1.  0.]\n",
      " [ 0. -1.  0.]]\n",
      "\n",
      "\n",
      "[26.618 26.016]\n",
      "[ 1. -1.]\n"
     ]
    }
   ],
   "source": [
    "#normalisation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_X = StandardScaler()\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "\n",
    "scaler_y = StandardScaler()\n",
    "y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).squeeze()\n",
    "\n",
    "print(X)\n",
    "print(X_scaled)\n",
    "print(\"\\n\")\n",
    "print(y)\n",
    "print(y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "X_scaled = torch.from_numpy(X_scaled.astype(np.float32))\n",
    "y_scaled = torch.from_numpy(y_scaled.astype(np.float32))\n",
    "\n",
    "print(X_scaled.shape)\n",
    "print(y_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_freq(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP_freq, self).__init__()\n",
    "        self.fc1 = nn.Linear(3, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(self.fc1(x))\n",
    "        x = F.tanh(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 - Loss: 1.4748\n",
      "Epoch 2/100 - Loss: 1.4686\n",
      "Epoch 3/100 - Loss: 1.4625\n",
      "Epoch 4/100 - Loss: 1.4564\n",
      "Epoch 5/100 - Loss: 1.4504\n",
      "Epoch 6/100 - Loss: 1.4444\n",
      "Epoch 7/100 - Loss: 1.4385\n",
      "Epoch 8/100 - Loss: 1.4327\n",
      "Epoch 9/100 - Loss: 1.4270\n",
      "Epoch 10/100 - Loss: 1.4213\n",
      "Epoch 11/100 - Loss: 1.4157\n",
      "Epoch 12/100 - Loss: 1.4102\n",
      "Epoch 13/100 - Loss: 1.4047\n",
      "Epoch 14/100 - Loss: 1.3993\n",
      "Epoch 15/100 - Loss: 1.3940\n",
      "Epoch 16/100 - Loss: 1.3887\n",
      "Epoch 17/100 - Loss: 1.3835\n",
      "Epoch 18/100 - Loss: 1.3784\n",
      "Epoch 19/100 - Loss: 1.3733\n",
      "Epoch 20/100 - Loss: 1.3683\n",
      "Epoch 21/100 - Loss: 1.3634\n",
      "Epoch 22/100 - Loss: 1.3585\n",
      "Epoch 23/100 - Loss: 1.3538\n",
      "Epoch 24/100 - Loss: 1.3490\n",
      "Epoch 25/100 - Loss: 1.3444\n",
      "Epoch 26/100 - Loss: 1.3398\n",
      "Epoch 27/100 - Loss: 1.3352\n",
      "Epoch 28/100 - Loss: 1.3308\n",
      "Epoch 29/100 - Loss: 1.3263\n",
      "Epoch 30/100 - Loss: 1.3220\n",
      "Epoch 31/100 - Loss: 1.3177\n",
      "Epoch 32/100 - Loss: 1.3135\n",
      "Epoch 33/100 - Loss: 1.3093\n",
      "Epoch 34/100 - Loss: 1.3051\n",
      "Epoch 35/100 - Loss: 1.3011\n",
      "Epoch 36/100 - Loss: 1.2970\n",
      "Epoch 37/100 - Loss: 1.2931\n",
      "Epoch 38/100 - Loss: 1.2891\n",
      "Epoch 39/100 - Loss: 1.2853\n",
      "Epoch 40/100 - Loss: 1.2815\n",
      "Epoch 41/100 - Loss: 1.2777\n",
      "Epoch 42/100 - Loss: 1.2739\n",
      "Epoch 43/100 - Loss: 1.2703\n",
      "Epoch 44/100 - Loss: 1.2666\n",
      "Epoch 45/100 - Loss: 1.2630\n",
      "Epoch 46/100 - Loss: 1.2594\n",
      "Epoch 47/100 - Loss: 1.2559\n",
      "Epoch 48/100 - Loss: 1.2524\n",
      "Epoch 49/100 - Loss: 1.2490\n",
      "Epoch 50/100 - Loss: 1.2456\n",
      "Epoch 51/100 - Loss: 1.2422\n",
      "Epoch 52/100 - Loss: 1.2388\n",
      "Epoch 53/100 - Loss: 1.2355\n",
      "Epoch 54/100 - Loss: 1.2322\n",
      "Epoch 55/100 - Loss: 1.2290\n",
      "Epoch 56/100 - Loss: 1.2258\n",
      "Epoch 57/100 - Loss: 1.2226\n",
      "Epoch 58/100 - Loss: 1.2194\n",
      "Epoch 59/100 - Loss: 1.2163\n",
      "Epoch 60/100 - Loss: 1.2132\n",
      "Epoch 61/100 - Loss: 1.2102\n",
      "Epoch 62/100 - Loss: 1.2071\n",
      "Epoch 63/100 - Loss: 1.2041\n",
      "Epoch 64/100 - Loss: 1.2011\n",
      "Epoch 65/100 - Loss: 1.1982\n",
      "Epoch 66/100 - Loss: 1.1952\n",
      "Epoch 67/100 - Loss: 1.1923\n",
      "Epoch 68/100 - Loss: 1.1894\n",
      "Epoch 69/100 - Loss: 1.1866\n",
      "Epoch 70/100 - Loss: 1.1837\n",
      "Epoch 71/100 - Loss: 1.1809\n",
      "Epoch 72/100 - Loss: 1.1782\n",
      "Epoch 73/100 - Loss: 1.1754\n",
      "Epoch 74/100 - Loss: 1.1727\n",
      "Epoch 75/100 - Loss: 1.1700\n",
      "Epoch 76/100 - Loss: 1.1673\n",
      "Epoch 77/100 - Loss: 1.1646\n",
      "Epoch 78/100 - Loss: 1.1620\n",
      "Epoch 79/100 - Loss: 1.1593\n",
      "Epoch 80/100 - Loss: 1.1567\n",
      "Epoch 81/100 - Loss: 1.1542\n",
      "Epoch 82/100 - Loss: 1.1516\n",
      "Epoch 83/100 - Loss: 1.1491\n",
      "Epoch 84/100 - Loss: 1.1466\n",
      "Epoch 85/100 - Loss: 1.1441\n",
      "Epoch 86/100 - Loss: 1.1416\n",
      "Epoch 87/100 - Loss: 1.1392\n",
      "Epoch 88/100 - Loss: 1.1368\n",
      "Epoch 89/100 - Loss: 1.1344\n",
      "Epoch 90/100 - Loss: 1.1320\n",
      "Epoch 91/100 - Loss: 1.1297\n",
      "Epoch 92/100 - Loss: 1.1273\n",
      "Epoch 93/100 - Loss: 1.1250\n",
      "Epoch 94/100 - Loss: 1.1228\n",
      "Epoch 95/100 - Loss: 1.1205\n",
      "Epoch 96/100 - Loss: 1.1183\n",
      "Epoch 97/100 - Loss: 1.1160\n",
      "Epoch 98/100 - Loss: 1.1139\n",
      "Epoch 99/100 - Loss: 1.1117\n",
      "Epoch 100/100 - Loss: 1.1095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Antoine\\anaconda3\\envs\\Baseline\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:610: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "model_frequ = MLP_freq()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model_frequ.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# Boucle d'entraînement\n",
    "for epoch in range(num_epochs):\n",
    "    model_frequ.train()  # mode entraînement \n",
    "\n",
    "    # Forward\n",
    "    pred = model_frequ(X_scaled)\n",
    "    loss = criterion(pred, y_scaled)\n",
    "\n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Affichage de la perte\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X = torch.from_numpy(X.astype(np.float32))\n",
    "y = torch.from_numpy(y.astype(np.float32))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Baseline",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
